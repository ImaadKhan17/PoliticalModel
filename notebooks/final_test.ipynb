{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf5d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30f03046",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../label_encoder.pkl', 'rb') as le:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe371ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1305.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c545a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PolitcalModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        # Initialize Longformer\n",
    "        super(PolitcalModel, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        # Create a dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        # Create a classification head (Linear layer that maps hidden dim -> num_classes)\n",
    "        self.topic_head = nn.Linear(self.model.config.hidden_size, num_classes )\n",
    "        # Create a regression head (Linear layer that maps hidden dim -> 1)\n",
    "        self.stance_head = nn.Linear(self.model.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass inputs through Longformer\n",
    "        outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        # Get the pooled output (usually from CLS token or mean of last layer)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        masked_embeddings = last_hidden_state * mask\n",
    "        x = masked_embeddings.sum(dim=1) / mask.sum(dim=1)\n",
    "        # Pass that to dropout\n",
    "        x = self.dropout(x)\n",
    "        # Pass into classification head → topic logits\n",
    "        topic_logits = self.topic_head(x)\n",
    "        # Pass into regression head → stance prediction\n",
    "        stance_pred = self.stance_head(x)\n",
    "        # Return both outputs\n",
    "        return topic_logits, stance_pred\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376fd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"allenai/longformer-base-4096\"\n",
    "num_classes = 199\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\")\n",
    "max_length = 1028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9484ced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PolitcalModel(model_name, num_classes).to(device)\n",
    "model.load_state_dict(torch.load(\"D:/Users/imaad/Documents/my-projects/political_app/politicalApp/models/best_model.pt\",weights_only = True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0238ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    model.eval()\n",
    "    encoded  = tokenizer(text, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors='pt' )\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predict(\"To give everyone free money \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee09a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dddf3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, pred = torch.max(b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7e9330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stance = a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "700f0535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([208.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([pred.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3209c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0125]], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stance*2)-1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pol_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
